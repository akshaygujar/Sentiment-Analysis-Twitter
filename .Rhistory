model5 = neuralnet(spam ~., train,linear.output=T)
View(train)
model5 = neuralnet(spam ~word_freq_make+word_freq_all, train,linear.output=T)
model5 = neuralnet(spam ~word_freq_make+word_freq_all, train, hidden = 10, linear.output=T)
table(mydata)
mydata = read.csv(file = "C:/Users/akshay.gujar/Desktop/Spam Detection.csv", header = TRUE)
table(mydata)
table(mydata$spam)
library('rvest')
install.packages('rvest')
library('rvest')
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'
webpage <- read_html(url)
url
webpage
install.packages("Rfacebook")
install.packages("RCurl")
install.packages("Rfacebook")
library(Rfacebook)
library(RCurl
)
install.packages("RCurl")
library(RCurl)
fb_oauth <- fbOAuth(app_id="1262869393762789", app_secret="b2c09ea07028a3fe011269703ec55627",extended_permissions = TRUE)
me <- getUsers("me",token=fb_oauth, private_info=TRUE)
me$name
me$name
likes = getLikes(user="me", token = fb_oauth)
sample(likes$names, 10)
likes = getLikes(user="me", token = fb_oauth)
sample(likes$names, )
updateStatus("Just do it", token=fb_oauth)
pages <- searchPages( string="trump", token=fb_oauth, n=200)
page <- getPage(page="Common Sense", token=fb_oauth, n=200)
page = getPage(page="Common Sense", token=fb_oauth, n=200)
page = getPage(page="Common Sense", token=fb_oauth, n=200)
install.packages("twitteR")
install.packages("RCurl")
require(twitteR)
require(RCurl)
library(RCurl)
library(twitteR)
install.packages("bitops")
install.packages("bitops")
install.packages("bitops")
install.packages("bitops")
install.packages("bitops")
install.packages("bitops")
require(twitteR)
require(RCurl)
consumer_key <- 'c4M4HJpMHmznrLRmSVyzoETE1'
consumer_secret <- '5TuSbC2TifnkvrUe1IyW0PGR91TZ8NafitTjaPwVHOlKRzATTt'
access_token <- '804564698515443712-rKr8KgcCzStnp5onK6jPQL1l5cuk4zo'
access_secret <- 'SihldVxSjMBpYZXmSYxihPUMBM8zDkDgXDyxI8kZPJjKq'
setup_twitter_oauth(consumer_key,consumer_secret, access_token, access_secret)
2
LFC_tweets <- searchTwitter("LFC", n=10, lang = "en")
LFC_tweets
str(LFC_tweets)
LFC_tweets[1:3]
install.packages("wordcloud")
require(wordcloud)
require(tm)
ML <- searchTwitter('Machine+Learning', lang = "en", n= 500, resultType = "recent")
class(ML)
ML_text <- sapply(ML, function(x) x$getText())
class(ML_text)
str(ML_text)
ML_text[36]
ML_text
ML_text <- ML_text[-36]
str(ML_text)
ML_corpus <- Corpus(VectorSource(ML_text))
ML_corpus
inspect(ML_corpus[1])
inspect(ML_corpus[100])
ML_clean <- tm_map(ML_corpus, removePunctuation)
ML_clean <- tm_map(ML_clean, content_transformer(tolower))
ML_clean <- tm_map(ML_clean, removeWords, stopwords("en"))
ML_clean <- tm_map(ML_clean, removeNumbers)
ML_clean <- tm_map(ML_clean, removeWords, c("MAchine", "Learning"))
wordcloud(ML_clean)
plot.new()
wordcloud(ML_clean)
wordcloud(ML_clean, random.order = F)
wordcloud(ML_clean, random.order = F, scale = c(3, 0.5))
wordcloud(ML_clean, random.order = F, scale = c(3, 0.5))
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5))
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5), col = red)
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5), col = redblue())
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5), col = redblue())
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5), col = "red"
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5), col = "red")
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5), col = "red")
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5), col = rainbow(50))
wordcloud(ML_clean, random.order = F, scale = c(6, 0.5), col = rainbow(50))
wordcloud(ML_clean, random.order = F, scale = c(6, 0.1), col = rainbow(50))
wordcloud(ML_clean, random.order = F, scale = c(6, 0.1), col = rainbow(30))
wordcloud(ML_clean, random.order = F, max.words = 40, scale = c(6, 0.1), col = rainbow(30))
wordcloud(ML_clean, random.order = F, max.words = 40, scale = c(6, 0.5), col = rainbow(30))
wordcloud(ML_clean, random.order = F, max.words = 40, col = rainbow(30))
wordcloud(ML_clean, random.order = F, col = rainbow(30))
library(twitteR)
library(ROAuth)
consumer_key <- "vn5HRzzx7rVvsVxsgHqIuO2RL"
consumer_secret <- "Ve5K2LcVR2t3c9xZsWa1si2t5iG8hPx8Mus2MuSdtyhzdDBOrv"
access_token <- "804564698515443712-LTAxs4YDBdkDindu6h21Ms0sJgiYNQp"
access_secret <- "1SPP8PPpSc5Pdz5DeNZfPf6bbR4Omj0wfK9A9hMlQOinC"
download.file(url = "http://curl.haxx.se/ca/cacert.pem", destfile = "cacert.pem")
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
cred <- OAuthFactory$new(consumerKey=consumer_key,
consumerSecret=consumer_secret, requestURL= 'https://api.twitter.com/oauth/request_token', accessURL='https://api.twitter.com/oauth/access_token', authURL='https://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
trump.tweets = searchTwitter("curency change narendra modi", n=1500)
trump.tweets
trump.tweets
class(trump.tweets)
trump.tweets = searchTwitter("curency change", n=1500)
trump.tweets = searchTwitter("narendra modi", n=1500)
trump.tweets
trump.tweets = searchTwitter("narendra modi currency change", n=1500)
trump.tweets
trump.tweets = searchTwitter(c("narendra modi", "currency change"), n=1500)
trump.tweets
trump.tweets[100]
trump.tweets[11]
trump.tweets = searchTwitter("narendra modi" && "currency change", n=1500)
trump.tweets = searchTwitter("narendra modi" & "currency change", n=1500)
trump.tweets = searchTwitter("narendra modi currency change", n=1500)
trump.tweets
trump.tweets = searchTwitter("narendra modi AND currency change", n=1500)
trump.tweets = searchTwitter("#narendra modi AND currency change", n=1500)
trump.tweets = searchTwitter("#narendramodi AND currency change", n=1500)
trump.tweets = searchTwitter("narendramodi AND currency change", n=1500)
trump.tweets
trump.tweets = searchTwitter("@narendramodi AND currency change", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change", n=1500)
trump.tweets
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia OR RBI AND currency change", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency OR old currency", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency OR Notes", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency OR Notes OR curruption", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency OR Notes OR Curruption", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency OR Notes OR Curruption OR change", n=1500)
trump.tweets
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency OR change", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency OR Currency", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency", n=1500)
trump.tweets = searchTwitter("narendramodi OR @narendramodi OR PMOindia AND currency change OR currency", n=1500)
df <- do.call("rbind", lapply(trump.tweets, as.data.frame))
df
df$text <- sapply(df$text,function(row) iconv(row, "latin1", "ASCII", sub="")) #remove emoticon
df$text = gsub("(f|ht)tp(s?)://(.*)[.][a-z]+", "", df$text) #remove URL
sample <- df$text
sample
pos.words = scan('positive-words.txt', what='character', comment.char=';') #Make sure you edit the location
neg.words = scan('negative-words.txt', what='character', comment.char=';')
pos.words
pos.words = scan('positive-words.txt', what='character', comment.char=';') #Make sure you edit the location
getwd
getwd()
setwd("C:/Users/akshay.gujar/Documents"/Twitter)
setwd("C:/Users/akshay.gujar/Documents/Twitter")
getwd()
pos.words = scan('positive-words.txt', what='character', comment.char=';') #Make sure you edit the location
neg.words = scan('negative-words.txt', what='character', comment.char=';')
pos.words=c(pos.words, 'Congrats', 'prizes', 'prize', 'thanks', 'thnx', 'Grt', 'gr8', 'plz', 'trending', 'recovering', 'brainstorm', 'leader')
neg.words = c(neg.words, 'Fight', 'fighting', 'wtf', 'arrest', 'no', 'not')
sample
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
list=lapply(sentences, function(sentence, pos.words, neg.words)
{
sentence = gsub('[[:punct:]]',' ',sentence)
sentence = gsub('[[:cntrl:]]','',sentence)
sentence = gsub('\\d+','',sentence)  #removes decimal number
sentence = gsub('\n','',sentence)    #removes new lines
sentence = tolower(sentence)
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)  #changes a list to character vector
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
pp = sum(pos.matches)
nn = sum(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
list1 = c(score, pp, nn)
return (list1)
}, pos.words, neg.words)
score_new = lapply(list, `[[`, 1)
pp1 = lapply(list, `[[`, 2)
nn1 = lapply(list, `[[`, 3)
scores.df = data.frame(score = score_new, text=sentences)
positive.df = data.frame(Positive = pp1, text=sentences)
negative.df = data.frame(Negative = nn1, text=sentences)
list_df = list(scores.df, positive.df, negative.df)
return(list_df)
}
result = score.sentiment(sample, pos.words, neg.words)
library(reshape)
result
test1=result[[1]]
test2=result[[2]]
test3=result[[3]]
test1$text=NULL
test2$text=NULL
test3$text=NULL
q1=test1[1,]
q2=test2[1,]
q3=test3[1,]
qq1=melt(q1, ,var='Score')
qq2=melt(q2, ,var='Positive')
qq3=melt(q3, ,var='Negative')
qq1['Score'] = NULL
qq2['Positive'] = NULL
qq3['Negative'] = NULL
table1 = data.frame(Text=result[[1]]$text, Score=qq1)
table2 = data.frame(Text=result[[2]]$text, Score=qq2)
table3 = data.frame(Text=result[[3]]$text, Score=qq3)
table_final=data.frame(Text=table1$Text, Score=table1$value, Positive=table2$value, Negative=table3$value)
table_final
posSc=table_final$Positive
negSc=table_final$Negative
table_final$PosPercent = posSc/ (posSc+negSc)
pp = table_final$PosPercent
pp[is.nan(pp)] <- 0
table_final$PosPercent = pp
table_final$NegPercent = negSc/ (posSc+negSc)
nn = table_final$NegPercent
nn[is.nan(nn)] <- 0
table_final$NegPercent = nn
table_final
hist(table_final$Positive, col=rainbow(10))
hist(table_final$Negative, col=rainbow(10))
hist(table_final$Score, col=rainbow(10))
slices <- c(sum(table_final$Positive), sum(table_final$Negative))
labels <- c("Positive", "Negative")
pie(slices, labels = labels, col = rainbow(length(labels)), main = "SEntiment Analysis")
library(plotrix)
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.00, main="Sentiment Analysis")
trump_text = sapply(trump.tweets, function(x) x$getText()) #sapply returns a vector
df <- do.call("rbind", lapply(trump.tweets, as.data.frame)) #lapply returns a list
trump_text <- sapply(df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
str(trump_text) #gives the summary/internal structure of an R object
library(tm) #tm: text mining
trump_corpus <- Corpus(VectorSource(trump_text)) #corpus is a collection of text documents
trump_corpus
inspect(trump_corpus[1])
library(wordcloud)
trump_clean <- tm_map(trump_corpus, removePunctuation)
trump_clean <- tm_map(trump_clean, removeWords, stopwords("english"))
trump_clean <- tm_map(trump_clean, removeNumbers)
trump_clean <- tm_map(trump_clean, stripWhitespace)
wordcloud(trump_clean, random.order=F,max.words=80, col=rainbow(50), scale=c(3.5,1))
library(twitteR)
tw = userTimeline("narendramodi", n = 3200)
rtw = twListToDF(tw)
tw = userTimeline("narendramodi", n = 3200)
tw = twListToDF(tw)
vec1 = tw$text
hash.pattern = "#[[:alpha:]]+"
have.hash = grep(x = vec1, pattern = hash.pattern) #stores the indices of the tweets which have hashes
hash.matches = gregexpr(pattern = hash.pattern,
text = vec1[have.hash])
extracted.hash = regmatches(x = vec1[have.hash], m = hash.matches) #the actual hashtags are stored here
df = data.frame(table(tolower(unlist(extracted.hash)))) #dataframe formed with var1(hashtag), freq of hashtag
colnames(df) = c("tag","freq")
df = df[order(df$freq,decreasing = TRUE),]
dat = head(df,50)
dat2 = transform(dat,tag = reorder(tag,freq)) #reorder it so that highest freq is at the top
library(ggplot2)
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = "blue")
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = "rainbow")
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow)
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow)
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = "blue")
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow(10))
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow(17))
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
trump.tweets = searchTwitter("Sunil Grover fight", n=1500)
trump.tweets
df <- do.call("rbind", lapply(trump.tweets, as.data.frame))
df$text <- sapply(df$text,function(row) iconv(row, "latin1", "ASCII", sub="")) #remove emoticon
df$text = gsub("(f|ht)tp(s?)://(.*)[.][a-z]+", "", df$text) #remove URL
sample <- df$text
test1$text=NULL
result = score.sentiment(sample, pos.words, neg.words)
library(reshape)
test1=result[[1]]
test2=result[[2]]
test3=result[[3]]
test1$text=NULL
test2$text=NULL
test3$text=NULL
q1=test1[1,]
q2=test2[1,]
q3=test3[1,]
qq1=melt(q1, ,var='Score')
qq2=melt(q2, ,var='Positive')
qq3=melt(q3, ,var='Negative')
qq1['Score'] = NULL
qq2['Positive'] = NULL
qq3['Negative'] = NULL
table1 = data.frame(Text=result[[1]]$text, Score=qq1)
table2 = data.frame(Text=result[[2]]$text, Score=qq2)
table3 = data.frame(Text=result[[3]]$text, Score=qq3)
table_final=data.frame(Text=table1$Text, Score=table1$value, Positive=table2$value, Negative=table3$value)
posSc=table_final$Positive
negSc=table_final$Negative
table_final$PosPercent = posSc/ (posSc+negSc)
pp = table_final$PosPercent
pp[is.nan(pp)] <- 0
table_final$PosPercent = pp
table_final$NegPercent = negSc/ (posSc+negSc)
nn = table_final$NegPercent
nn[is.nan(nn)] <- 0
table_final$NegPercent = nn
hist(table_final$Positive, col=rainbow(10))
hist(table_final$Negative, col=rainbow(10))
hist(table_final$Score, col=rainbow(10))
slices <- c(sum(table_final$Positive), sum(table_final$Negative))
labels <- c("Positive", "Negative")
pie(slices, labels = labels, col = rainbow(length(labels)), main = "SEntiment Analysis")
library(plotrix)
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.00, main="Sentiment Analysis")
trump_text = sapply(trump.tweets, function(x) x$getText()) #sapply returns a vector
df <- do.call("rbind", lapply(trump.tweets, as.data.frame)) #lapply returns a list
trump_text <- sapply(df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
str(trump_text) #gives the summary/internal structure of an R object
library(tm) #tm: text mining
trump_corpus <- Corpus(VectorSource(trump_text)) #corpus is a collection of text documents
trump_corpus
inspect(trump_corpus[1])
library(wordcloud)
trump_clean <- tm_map(trump_corpus, removePunctuation)
trump_clean <- tm_map(trump_clean, removeWords, stopwords("english"))
trump_clean <- tm_map(trump_clean, removeNumbers)
trump_clean <- tm_map(trump_clean, stripWhitespace)
wordcloud(trump_clean, random.order=F,max.words=80, col=rainbow(50), scale=c(3.5,1))
tw = userTimeline("thekapilsharmashow", n = 3200)
tw = twListToDF(tw)
vec1 = tw$text
hash.pattern = "#[[:alpha:]]+"
have.hash = grep(x = vec1, pattern = hash.pattern) #stores the indices of the tweets which have hashes
hash.matches = gregexpr(pattern = hash.pattern,
text = vec1[have.hash])
extracted.hash = regmatches(x = vec1[have.hash], m = hash.matches) #the actual hashtags are stored here
df = data.frame(table(tolower(unlist(extracted.hash)))) #dataframe formed with var1(hashtag), freq of hashtag
colnames(df) = c("tag","freq")
df = df[order(df$freq,decreasing = TRUE),]
dat = head(df,50)
dat2 = transform(dat,tag = reorder(tag,freq)) #reorder it so that highest freq is at the top
library(ggplot2)
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow(17))
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
tw = userTimeline("thekapilsharmashow", n = 3200)
tw = userTimeline("kapilsharma", n = 3200)
tw = twListToDF(tw)
vec1 = tw$text
hash.pattern = "#[[:alpha:]]+"
have.hash = grep(x = vec1, pattern = hash.pattern) #stores the indices of the tweets which have hashes
hash.matches = gregexpr(pattern = hash.pattern,
text = vec1[have.hash])
extracted.hash = regmatches(x = vec1[have.hash], m = hash.matches) #the actual hashtags are stored here
df = data.frame(table(tolower(unlist(extracted.hash)))) #dataframe formed with var1(hashtag), freq of hashtag
colnames(df) = c("tag","freq")
df = df[order(df$freq,decreasing = TRUE),]
dat = head(df,50)
dat2 = transform(dat,tag = reorder(tag,freq)) #reorder it so that highest freq is at the top
library(ggplot2)
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow(17))
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
tw = userTimeline("kapilsharmashow", n = 3200)
tw = twListToDF(tw)
vec1 = tw$text
hash.pattern = "#[[:alpha:]]+"
have.hash = grep(x = vec1, pattern = hash.pattern) #stores the indices of the tweets which have hashes
hash.matches = gregexpr(pattern = hash.pattern,
text = vec1[have.hash])
extracted.hash = regmatches(x = vec1[have.hash], m = hash.matches) #the actual hashtags are stored here
df = data.frame(table(tolower(unlist(extracted.hash)))) #dataframe formed with var1(hashtag), freq of hashtag
colnames(df) = c("tag","freq")
df = df[order(df$freq,decreasing = TRUE),]
dat = head(df,50)
dat2 = transform(dat,tag = reorder(tag,freq)) #reorder it so that highest freq is at the top
library(ggplot2)
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow(17))
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow(7))
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
tw = userTimeline("sunil grover fight", n = 3200)
tw = userTimeline("sunilgrover", n = 3200)
tw = twListToDF(tw)
vec1 = tw$text
hash.pattern = "#[[:alpha:]]+"
hash.matches = gregexpr(pattern = hash.pattern,
text = vec1[have.hash])
have.hash = grep(x = vec1, pattern = hash.pattern) #stores the indices of the tweets which have hashes
extracted.hash = regmatches(x = vec1[have.hash], m = hash.matches) #the actual hashtags are stored here
df = data.frame(table(tolower(unlist(extracted.hash)))) #dataframe formed with var1(hashtag), freq of hashtag
colnames(df) = c("tag","freq")
df = df[order(df$freq,decreasing = TRUE),]
dat = head(df,50)
dat2 = transform(dat,tag = reorder(tag,freq)) #reorder it so that highest freq is at the top
library(ggplot2)
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow(7))
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
hash.matches = gregexpr(pattern = hash.pattern, text = vec1[have.hash])
extracted.hash = regmatches(x = vec1[have.hash], m = hash.matches) #the actual hashtags are stored here
df = data.frame(table(tolower(unlist(extracted.hash)))) #dataframe formed with var1(hashtag), freq of hashtag
colnames(df) = c("tag","freq")
df = df[order(df$freq,decreasing = TRUE),]
colnames(df) = c("tag","freq")
df = df[order(df$freq,decreasing = TRUE),]
dat = head(df,50)
dat2 = transform(dat,tag = reorder(tag,freq)) #reorder it so that highest freq is at the top
library(ggplot2)
p = ggplot(dat2, aes(x = tag, y = freq)) + geom_bar(stat="identity", fill = rainbow(7))
p + coord_flip() + labs(title = "Hashtag frequencies in the tweets of the Obama team (@BarackObama)")
tw = userTimeline("Sunil Grover fight", n = 3200)
hist(table_final$Positive, col=rainbow(10), main = "Positive Score")
hist(table_final$Positive, col=rainbow(10), main = "Positive Score", xlab = "Ranks")
hist(table_final$Positive, col=rainbow(10), main = "Positive Score", xlab = "Ranks", ylab = "# tweets showing positive feedback")
hist(table_final$Negative, col=rainbow(10),  main = "Nagetive Score", xlab = "Ranks", ylab = "# tweets showing negitive feedback")
hist(table_final$Score, col=rainbow(10),  main = "Score", xlab = "Rank", ylab = "# tweets showing positive & negative feedback")
hist(table_final$Score, col=rainbow(10),  main = "Score", xlab = "Rank", ylab = "# tweets")
hist(table_final$Score, col=rainbow(10),  main = "Score", xlab = "Rank", ylab = "# tweets")
hist(table_final$Score, col=rainbow(length(table_final$Score)),  main = "Score", xlab = "Rank", ylab = "# tweets")
hist(table_final$Score, col=rainbow(length(table_final$Score)),  main = "Score", xlab = "Rank", ylab = "# tweets")
10
hist(table_final$Score, col=rainbow(10),  main = "Score", xlab = "Rank", ylab = "# tweets")
slices <- c(sum(table_final$Positive), sum(table_final$Negative))
labels <- c("Positive", "Negative")
pie(slices, labels = labels, col = rainbow(length(labels)), main = "SEntiment Analysis")
library(plotrix)
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.00, main="Sentiment Analysis")
pct <- round(slices/sum(slices)*100)
labels <- paste(labels, pct) # add percents to labels
labels <- paste(labels,"%",sep="") # ad % to labels
pie(slices, labels = labels, col = rainbow(length(labels)), main = "SEntiment Analysis")
library(plotrix)
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.00, main="Sentiment Analysis")
hist(table_final$Score, col=rainbow(10),  main = "Score", xlab = "<-Negative  Rank  Positive->", ylab = "# tweets")
hist(table_final$Score, col=rainbow(10),  main = "Score based on teitter", xlab = "<-Negative  Score  Positive->", ylab = "# tweets")
hist(table_final$Score, col=rainbow(10),  main = "Score based on twitter", xlab = "<-Negative  Score  Positive->", ylab = "# tweets")
hist(table_final$Score, col=rainbow(10),  main = "Tweet Score of Sunil Grover fight", xlab = "<-Negative  Score  Positive->", ylab = "# tweets")
hist(table_final$Score, col=rainbow(10),  main = "Sentiment Analysis of Sunil_Grover_fight", xlab = "<-Negative  Score  Positive->", ylab = "# tweets")
pie(slices, labels = labels, col = rainbow(length(labels)), main = "Sentiment Analysis")
pie(slices, labels = labels, col = rainbow(length(labels)), main = "Sentiment Analysis of Sunil_Grover_fight")
wordcloud(trump_clean, random.order=F,max.words=80, col=rainbow(50), scale=c(3.5,1))
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.01, main="Sentiment Analysis")
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.05, main="Sentiment Analysis")
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.07, main="Sentiment Analysis")
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.07, main="Sentiment Analysis of Sunil_Grover_fight")
pie3D(slices, labels = labels, col=c("red", "blue"),explode=0.07, main="Sentiment Analysis of Sunil_Grover_fight")
pie3D(slices, labels = labels, col=c("blue", "blue"),explode=0.07, main="Sentiment Analysis of Sunil_Grover_fight")
pie3D(slices, labels = labels, col=c("blue", "red"),explode=0.07, main="Sentiment Analysis of Sunil_Grover_fight")
pie3D(slices, labels = labels, col=c("green", "red"),explode=0.07, main="Sentiment Analysis of Sunil_Grover_fight")
pie(slices, labels = labels, col=c("green", "red"), main = "Sentiment Analysis of Sunil_Grover_fight")
library(plotrix)
table_final
some_txt <- table_final$Text
install.packages("syuzhet")
library(syuzhet)
mysentiment<-get_nrc_sentiment((some_txt))
some_txt <- as.character(table_final$Text)
library(syuzhet)
mysentiment<-get_nrc_sentiment((some_txt))
mysentiment
mysentiment.positive =sum(mysentiment$positive)
mysentiment.anger =sum(mysentiment$anger)
mysentiment.anticipation =sum(mysentiment$anticipation)
mysentiment.disgust =sum(mysentiment$disgust)
mysentiment.fear =sum(mysentiment$fear)
mysentiment.joy =sum(mysentiment$joy)
mysentiment.sadness =sum(mysentiment$sadness)
mysentiment.surprise =sum(mysentiment$surprise)
mysentiment.trust =sum(mysentiment$trust)
mysentiment.negative =sum(mysentiment$negative)
yAxis <- c(mysentiment.positive,
+ mysentiment.anger,
+ mysentiment.anticipation,
+ mysentiment.disgust,
+ mysentiment.fear,
+ mysentiment.joy,
+ mysentiment.sadness,
+ mysentiment.surprise,
+ mysentiment.trust,
+ mysentiment.negative)
xAxis <- c("Positive","Anger","Anticipation","Disgust","Fear","Joy","Sadness","Surprise","Trust","Negative")
colors <- c("green","red","blue","orange","red","green","orange","blue","green","red")
yRange <- range(0,yAxis) + 1000
barplot(yAxis, names.arg = xAxis,
xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment for Movie Rangoon 2017", sub = "Feb 2017", col = colors, border = "black", ylim = yRange, xpd = F, axisnames = T, cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue")
colSums(mysentiment)
barplot(yAxis, names.arg = xAxis, xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment for Movie Rangoon 2017", col = colors, border = "black", ylim = yRange, xpd = F, axisnames = T, cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue")
sub = "Feb 2017",
barplot(yAxis, names.arg = xAxis, xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment for Movie Rangoon 2017", sub = "Feb 2017", col = colors, border = "black", ylim = yRange, xpd = F, axisnames = T, cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue")
barplot(yAxis, names.arg = xAxis, xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment for Movie Rangoon 2017", col = colors, border = "black", ylim = yRange, xpd = F, axisnames = T, cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue")
barplot(yAxis, names.arg = xAxis, xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment for Movie Rangoon 2017", col = colors, border = "black", xpd = F, axisnames = T, cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue")
barplot(yAxis, names.arg = xAxis, xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment for Movie Rangoon 2017", col = colors, border = "black", axisnames = T, cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue")
barplot(yAxis, names.arg = xAxis, xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment for Movie Rangoon 2017", col = colors, border = "black", cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue")
barplot(yAxis, names.arg = xAxis, xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment for Sunil_Grover_Fight", col = colors, border = "black", cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue")
